{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'like', 'to', 'talk', 'today', 'about', 'how', 'to', 'develop', 'a', 'new', 'foreign', 'policy', 'direction', 'for', 'our', 'country', 'one', 'that', 'replaces', 'randomness', 'with', 'purpose', 'ideology', 'with', 'strategy', 'and', 'chaos', 'with', 'peace', 'it', 'is', 'time', 'to', 'shake', 'the', 'rust', 'off', 'of']\n",
      "1681\n",
      "10229\n",
      "15\n",
      "re\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1681\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print(len(d))\n",
    "max_word_length = max([len(id_to_token[np.argmax(j)]) for j in d])\n",
    "print(max_word_length)\n",
    "print(id_to_token[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_to_i, embedding = data.get_char_embedding()\n",
    "\n",
    "def word_generator(ix, num=10):\n",
    "    return d[ix : ix + num]\n",
    "\n",
    "def sequence_char_matrix(data, ix, num = 10):\n",
    "    words = [id_to_token[np.argmax(j)] for j in data[ix : ix + num]]\n",
    "    chars = [embedding[[c_to_i[c] for c in w]] for w in words]\n",
    "    to_pad = [max_word_length - t for t in map(lambda x: len(x), words)]\n",
    "    pad = [np.zeros([tp, embedding.shape[0]]) for tp in to_pad]\n",
    "    reshape = [1, max_word_length, embedding.shape[0], 1]\n",
    "    padded = np.concatenate([np.reshape(np.r_[ch, pd], reshape) for ch, pd in zip(chars, pad)], axis=0)\n",
    "    return padded\n",
    "\n",
    "a = sequence_char_matrix([d[0], d[0]], 0, 2)\n",
    "_ = word_generator(0, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 87\n"
     ]
    }
   ],
   "source": [
    "# CNN hyperparameters\n",
    "input_height = max_word_length\n",
    "input_width = embedding.shape[0]\n",
    "batch_size = 13\n",
    "print(input_height, input_width)\n",
    "filter_heights = [2, 3, 4]\n",
    "feature_maps = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(inp):\n",
    "    initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1, dtype=tf.float32)\n",
    "    with tf.variable_scope('conv1') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[0], input_width, 1, feature_maps[0]], initializer=initializer)\n",
    "        conv1 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool1 = tf.squeeze(tf.nn.max_pool(conv1, ksize=[1, conv1.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'), squeeze_dims=[1, 2])\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[1], input_width, 1, feature_maps[1]], initializer=initializer)\n",
    "        conv2 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool2 = tf.squeeze(tf.nn.max_pool(conv2, ksize=[1, conv2.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'), squeeze_dims=[1, 2])\n",
    "    \n",
    "    with tf.variable_scope('conv3') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[2], input_width, 1, feature_maps[2]], initializer=initializer)\n",
    "        conv3 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool3 = tf.squeeze(tf.nn.max_pool(conv3, ksize=[1, conv3.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'), squeeze_dims=[1, 2])\n",
    "    pool_total = tf.concat(1, [pool1, pool2, pool3], name='total_pool')\n",
    "    return pool_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_ = tf.placeholder(shape=[None, max_word_length, embedding.shape[0], 1], dtype=tf.float32, name = 'cnn_in')\n",
    "with tf.variable_scope(\"CNN\") as scope:\n",
    "    cnn_output = inference(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# highway network\n",
    "initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1, dtype=tf.float32)\n",
    "# initializer = tf.random_normal_initializer(mean=0, stddev=0.001, dtype=tf.float32)\n",
    "# transformation gate parameters\n",
    "W_t = tf.get_variable('Wt', shape=[sum(feature_maps)], initializer=initializer)\n",
    "b_t = tf.get_variable('bt', shape=[sum(feature_maps)], initializer=initializer)\n",
    "# highway parameters\n",
    "W_h = tf.get_variable('Wh', shape=[sum(feature_maps)], initializer=initializer)\n",
    "b_h = tf.get_variable('bh', shape=[sum(feature_maps)], initializer=initializer)\n",
    "# one layer highway network\n",
    "t = tf.nn.sigmoid(W_t * cnn_output + b_t)\n",
    "highway_output = t * tf.nn.tanh(W_h * cnn_output + b_h) + (tf.ones_like(t) -  t) * cnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1681)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN hyperparameters\n",
    "\n",
    "epochs = 5000\n",
    "hidden_layer = 128\n",
    "input_size = sum(feature_maps)\n",
    "output_size = vocab_size\n",
    "learning_rate = 0.1\n",
    "print_step = 1000\n",
    "num_steps = batch_size\n",
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1, dtype=tf.float32)\n",
    "initializer = tf.random_uniform_initializer(minval=-0.1, maxval=0.1, dtype=tf.float32)\n",
    "Wxh = tf.get_variable('Wxh', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whh = tf.get_variable('Whh', shape=[hidden_layer, hidden_layer], initializer=initializer)\n",
    "Why = tf.get_variable('Why',shape=[hidden_layer, output_size], initializer=initializer)\n",
    "by = tf.get_variable('by', shape=[output_size], initializer=initializer)\n",
    "# weights associated with update gate\n",
    "Wxz = tf.get_variable('Wxz', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whz = tf.get_variable('Whz', shape=[hidden_layer, hidden_layer], initializer=initializer)\n",
    "# weights associated with the reset gate\n",
    "Wxr = tf.get_variable('Wxr', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whr = tf.get_variable('Whr', shape=[hidden_layer, hidden_layer], initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GRU(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    z = tf.nn.sigmoid(tf.matmul(i, Wxz) + tf.matmul(p, Whz))    # update gate\n",
    "    r = tf.nn.sigmoid(tf.matmul(i, Wxr) + tf.matmul(p, Whr))    # reset gate\n",
    "    h_ = tf.nn.tanh(tf.matmul(i, Wxh) + tf.matmul(tf.mul(p, r), Whh))\n",
    "    h = tf.mul(tf.sub(tf.ones_like(z), z), h_) + tf.mul(z, p)\n",
    "    return tf.reshape(h, [hidden_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = tf.placeholder(shape=[None, vocab_size], dtype=tf.float32, name='targets')\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(GRU, highway_output, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by, name='model_out')\n",
    "# loss = -tf.reduce_mean(b * tf.log(outputs))\n",
    "loss =  - tf.reduce_mean(tf.reduce_sum(b * tf.log(outputs), 1))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(ix, num_steps):    \n",
    "    seed = word_generator(ix, 1)\n",
    "    text = [np.argmax(seed[0])]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(num_steps):\n",
    "        feed = {input_: np.array(sequence_char_matrix(seed, 0, 1)), initial: h}\n",
    "        out, e = sess.run([outputs, states], feed_dict=feed)\n",
    "        text.append(np.argmax(out[0]))\n",
    "        seed = out\n",
    "        h = e[-1]\n",
    "    return [id_to_token[j] for j in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "ix = 0\n",
    "sess.run(tf.initialize_all_variables())\n",
    "losses = []\n",
    "h = np.zeros(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "3.07657\n",
      "Epoch 1000\n",
      "3.29876\n",
      "Epoch 2000\n",
      "2.51236\n",
      "Epoch 3000\n",
      "2.30238\n",
      "Epoch 4000\n",
      "1.96116\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    if ix + num_steps >= len(d):\n",
    "        ix = 0\n",
    "    x = np.array(sequence_char_matrix(d, ix, batch_size))\n",
    "    y = np.array(word_generator(ix + 1, batch_size))\n",
    "    feed = {input_: x, initial: np.zeros(hidden_layer), b: y}    \n",
    "    o, l, _ = sess.run([outputs, loss, optimize_op], feed_dict=feed)\n",
    "    ix += num_steps\n",
    "    losses.append(l)\n",
    "    if i % print_step == 0:\n",
    "        print('Epoch', i)\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'year', 'that', 's', 'going', 'to', 'have', 'people', 'that', 's', 'the', 'disaster', 'that', 's', 'coming', 'nice', 'in', 'the', 'other', 'way', 'the', 'bridge', 'trade', 'off', 'our', 'country', 'we', 've', 'been', 'allowed', 'that', 'aren', 't', 'close', 'him', 'from', 'who', 'is', 'happening', 'and', 'now', 'we', 'have', 'to', 'do', 'you', 'know', 'we', 'have', 'a', 'political', 'political', 'he', 'said', 'it', 's', 'a', 'nation', 'of', 'it', 'said', 'me', 'wanted', 'but', 'we', 'will', 'be', 'accused', 'i', 'said', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(generate(10, 70))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
