{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'would', 'like', 'to', 'talk', 'today', 'about', 'how', 'to', 'develop', 'a', 'new', 'foreign', 'policy', 'direction', 'for', 'our', 'country', 'one', 'that', 'replaces', 'randomness', 'with', 'purpose', 'ideology', 'with', 'strategy', 'and', 'chaos', 'with', 'peace', 'it', 'is', 'time', 'to', 'shake', 'the', 'rust', 'off', 'of']\n",
      "1681\n",
      "10229\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1681\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print(len(d))\n",
    "x, y = d[10:20], d[11:21]\n",
    "max_word_length = max([len(id_to_token[np.argmax(j)]) for j in d])\n",
    "print(max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_to_i, embedding = data.get_char_embedding()\n",
    "\n",
    "def word_generator(ix, num=10):\n",
    "    return d[ix : ix + num]\n",
    "\n",
    "def sequence_char_matrix(ix, num = 10):\n",
    "    # returns char matrices of all words in a sequence starting from ix and of length num    \n",
    "    words = [id_to_token[np.argmax(j)] for j in d[ix : ix + num]]\n",
    "    chars = [embedding[[c_to_i[c] for c in w]] for w in words]\n",
    "    to_pad = [max_word_length - t for t in map(lambda x: len(x), words)]\n",
    "    pad = [np.zeros([tp, embedding.shape[0]]) for tp in to_pad]\n",
    "    reshape = [1, max_word_length, embedding.shape[0], 1]\n",
    "    padded = np.concatenate([np.reshape(np.r_[ch, pd], reshape) for ch, pd in zip(chars, pad)], axis=0)\n",
    "    return padded\n",
    "\n",
    "_ = sequence_char_matrix(3, 13)\n",
    "_ = word_generator(0, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 87\n"
     ]
    }
   ],
   "source": [
    "# CNN hyperparameters\n",
    "input_height = max_word_length\n",
    "input_width = embedding.shape[0]\n",
    "batch_size = 13\n",
    "print(input_height, input_width)\n",
    "filter_heights = [2, 3, 4]\n",
    "feature_maps = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(inp):\n",
    "    with tf.variable_scope('conv1') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[0], input_width, 1, feature_maps[0]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv1 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool1 = tf.squeeze(tf.nn.max_pool(conv1, ksize=[1, conv1.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[1], input_width, 1, feature_maps[1]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv2 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool2 = tf.squeeze(tf.nn.max_pool(conv2, ksize=[1, conv2.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    with tf.variable_scope('conv3') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[2], input_width, 1, feature_maps[2]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv3 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool3 = tf.squeeze(tf.nn.max_pool(conv3, ksize=[1, conv3.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    pool_total = tf.concat(0, [pool1, pool2, pool3], name='total_pool')\n",
    "    return pool_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_ = tf.placeholder(shape=[batch_size, max_word_length, embedding.shape[0], 1], dtype=tf.float32, name = 'cnn_in')\n",
    "words = tf.split(0, batch_size, input_)\n",
    "cnn_outputs = []\n",
    "with tf.variable_scope(\"CNN\") as scope:\n",
    "    for idx, word in enumerate(words):\n",
    "#         inp = tf.Variable(word, trainable=False, dtype=tf.float32)\n",
    "        if idx != 0:\n",
    "            scope.reuse_variables()\n",
    "        p = inference(word)\n",
    "        cnn_outputs.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# highway network\n",
    "highway_outputs = []\n",
    "initializer = tf.random_uniform_initializer(minval=-0.05, maxval=0.05, dtype=tf.float32)\n",
    "# transformation gate parameters\n",
    "W_t = tf.get_variable('Wt', shape=[sum(feature_maps)], initializer=initializer)\n",
    "b_t = tf.get_variable('bt', shape=[sum(feature_maps)], initializer=initializer)\n",
    "# highway parameters\n",
    "W_h = tf.get_variable('Wh', shape=[sum(feature_maps)], initializer=initializer)\n",
    "b_h = tf.get_variable('bh', shape=[sum(feature_maps)], initializer=initializer)\n",
    "# one layer highway network\n",
    "for co in cnn_outputs:\n",
    "    t = tf.nn.sigmoid(W_t * co + b_t)\n",
    "    z = t * tf.nn.tanh(W_h * co + b_h) + (tf.ones_like(t) -  t) * co\n",
    "    highway_outputs.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 1681)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN hyperparameters\n",
    "\n",
    "epochs = 7000\n",
    "hidden_layer = 128\n",
    "input_size = sum(feature_maps)\n",
    "output_size = vocab_size\n",
    "learning_rate = 0.1\n",
    "num_steps = batch_size\n",
    "print_step = 500\n",
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initializer = tf.random_uniform_initializer(minval=-0.05, maxval=0.05, dtype=tf.float32)\n",
    "Wxh = tf.get_variable('Wxh', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whh = tf.get_variable('Whh', shape=[hidden_layer, hidden_layer], initializer=initializer)\n",
    "Why = tf.get_variable('Why',shape=[hidden_layer, output_size], initializer=initializer)\n",
    "# weights associated with update gate\n",
    "Wxz = tf.get_variable('Wxz', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whz = tf.get_variable('Whz', shape=[hidden_layer, hidden_layer], initializer=initializer)\n",
    "# weights associated with the reset gate\n",
    "Wxr = tf.get_variable('Wxr', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whr = tf.get_variable('Whr', shape=[hidden_layer, hidden_layer], initializer=initializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GRU(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    z = tf.nn.sigmoid(tf.matmul(i, Wxz) + tf.matmul(p, Whz))    # update gate\n",
    "    r = tf.nn.sigmoid(tf.matmul(i, Wxr) + tf.matmul(p, Whr))    # reset gate\n",
    "    h_ = tf.nn.tanh(tf.matmul(i, Wxh) + tf.matmul(tf.mul(p, r), Whh))\n",
    "    h = tf.mul(tf.sub(tf.ones_like(z), z), h_) + tf.mul(z, p)\n",
    "#     h = zoneout(h, p)\n",
    "    return tf.reshape(h, [hidden_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = tf.placeholder(shape=[batch_size, vocab_size], dtype=tf.float32, name='targets')\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(GRU, highway_outputs, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why), name='model_out')\n",
    "loss = -tf.reduce_sum(b * tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x = np.array(sequence_char_matrix(ix, 1))\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(n):\n",
    "        o, h = sess.run([outputs, states], {a:x, initial: h})\n",
    "        h = h.reshape(hidden_layer)\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * input_size\n",
    "        x[o] = 1\n",
    "#         print np.argmax(x)\n",
    "        x = [x]\n",
    "    print(' '.join(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "ix = 0\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.5508\n",
      "59.2356\n",
      "35.8521\n",
      "8.5517\n",
      "5.55948\n",
      "3.48802\n",
      "8.50309\n",
      "2.92953\n",
      "3.24301\n",
      "2.84464\n",
      "1.56812\n",
      "0.737502\n",
      "0.970936\n",
      "1.44722\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    if ix + num_steps >= len(d):\n",
    "        ix = 0\n",
    "    x = np.array(sequence_char_matrix(ix, batch_size))\n",
    "    y = np.array(word_generator(ix, batch_size))\n",
    "    feed = {input_: x, initial: np.zeros(hidden_layer), b: y}    \n",
    "    o, l, _ = sess.run([outputs, loss, optimize_op], feed_dict=feed)\n",
    "    ix += num_steps\n",
    "    if i % print_step == 0:\n",
    "        print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
