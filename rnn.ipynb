{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from random import randint\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'in', u'the', u'beginning', u'god', u'created', u'the', u'heaven', u'and', u'the', u'earth', u'.', u'and', u'the', u'earth', u'was', u'without', u'form', u',', u'and', u'void', u';', u'and', u'darkness', u'was', u'upon', u'the', u'face', u'of', u'the', u'deep', u'.', u'and', u'the', u'spirit', u'of', u'god', u'moved', u'upon', u'the', u'face']\n",
      "1909\n",
      "29602\n",
      "[5, 1, 3, 45, 24, 466, 1421, 2, 1, 1131]\n",
      "[1, 3, 45, 24, 466, 1421, 2, 1, 1131, 9]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1909\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print len(d)\n",
    "x, y = d[10:20], d[11:21]\n",
    "print [np.argmax(j) for j in x]\n",
    "print [np.argmax(j) for j in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "input_size = output_size = vocab_size\n",
    "iterations = 10000\n",
    "hidden_layer = 128\n",
    "inp_out_size = vocab_size\n",
    "learning_rate = 0.1\n",
    "num_steps = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "initializer = tf.random_normal_initializer(mean=0, stddev=0.001, dtype=tf.float32)\n",
    "Wxh = tf.get_variable('Wxh', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whh = tf.get_variable('Whh', shape=[hidden_layer, hidden_layer], initializer=initializer)\n",
    "Why = tf.get_variable('Why',shape=[hidden_layer, output_size], initializer=initializer)\n",
    "by = tf.get_variable('by', shape=[output_size], initializer=initializer)\n",
    "# weights associated with update gate\n",
    "Wxz = tf.get_variable('Wxz', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whz = tf.get_variable('Whz', shape=[hidden_layer, hidden_layer], initializer=initializer)\n",
    "# weights associated with the reset gate\n",
    "Wxr = tf.get_variable('Wxr', shape=[input_size, hidden_layer], initializer=initializer)\n",
    "Whr = tf.get_variable('Whr', shape=[hidden_layer, hidden_layer], initializer=initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    z = tf.nn.sigmoid(tf.matmul(i, Wxz) + tf.matmul(p, Whz))    # update gate\n",
    "    r = tf.nn.sigmoid(tf.matmul(i, Wxr) + tf.matmul(p, Whr))    # reset gate\n",
    "    h_ = tf.nn.tanh(tf.matmul(i, Wxh) + tf.matmul(tf.mul(p, r), Whh))\n",
    "    h = tf.mul(tf.sub(tf.ones_like(z), z), h_) + tf.mul(z, p)\n",
    "    return tf.reshape(h, [hidden_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b*tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -4, 4), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)\n",
    "\n",
    "# optimize_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x, _ = data.sample(d, 1)\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(n):\n",
    "        o, h = sess.run([outputs, states], {a:x, initial: h})\n",
    "        h = h.reshape(hidden_layer)\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * inp_out_size\n",
    "        x[o] = 1\n",
    "#         print np.argmax(x)\n",
    "        x = [x]\n",
    "    print ' '.join(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "ix = 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if already trained previously, just restore\n",
    "to_restore = False\n",
    "\n",
    "if to_restore:\n",
    "    saver.restore(sess, 'model.ckpt')\n",
    "else:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print epochs\n",
    "h = np.zeros(hidden_layer)\n",
    "for i in range(iterations):\n",
    "#     x, y = data.sample(d, num_steps)\n",
    "    if ix + num_steps >= len(d):\n",
    "        ix = 0\n",
    "        print('one epoch complete')\n",
    "    h = np.zeros(hidden_layer)\n",
    "    x, y = d[ix : ix + num_steps], d[ix + 1 : ix + num_steps + 1]   \n",
    "    l, o, _ = sess.run([loss, outputs, optimize_op], {a: x, b: y, initial: h}) \n",
    "    smooth_loss = smooth_loss * 0.999 + l * 0.001\n",
    "    if i % 1000 == 0:\n",
    "        print('iteration {0}, loss = {1}'.format(i, l))\n",
    "    ix += num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving model\n",
    "# saver.save(sess, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "began to be his wife ; then she went down to her , and she conceived , and bare jacob a son son , and said , she hath taken away my blessing , i have made him thee . and he said , i am thy made , my lord hath blessed me , who hath also born me thee thy brother , that he may bless thee , and she also and her . and she conceived again , and bare a son , and said , i will surely give the god of my father give it , and to my master , when thou shalt have seen thy face , and thy cattle , and my soul shall live : and i will make thee my son , and let thee go : and we have not touched thee . and if we will bless thee , and let me drink also , and we will dwell with thee , and i will go thee my : and god saw that thou shalt be now unto me , and let my brethren , let us go : now i am speckled , and i will make thee also : let us make our daughters . and the daughters of my father 's serve with thee . and they said unto them , we be the daughter of my father , and hath given jacob 's and drink , and go with him . and they did eat and drink . and she conceived again , and bare jacob son , and said unto him , give me the daughter of my father , and they sware unto me , and to me for her ; and tell me , and i will deal cattle before thee . and they bare children me , and my soul shall be as we have done goats . and they went out out with this day . and they said unto her , we have given my master , and sware to me , and changed my wages : for i have served thee , and we have both cattle , and we have i will also . and they shall say , my cattle : and i have seen my face , and all that as we have both . and cattle , and pursued one of : and the land of my father 's day . and god came to day , and brought them unto his father , and let it not speckled and laban , and she goats , and ringstraked , speckled , and over all that thou shalt serve me . and jacob went out to me , and she goats that night , and i did eat . and the sons of jacob came from the day of the waters of . and the waters were abated from off the earth : and noah was so . and the lord appeared unto him and\n"
     ]
    }
   ],
   "source": [
    "generate(sess, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess.run(Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
