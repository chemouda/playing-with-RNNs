{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from random import randint\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "735\n",
      "[29, 56, 8, 1, 82, 9, 45, 22, 24, 67]\n",
      "[56, 8, 1, 82, 9, 45, 22, 24, 67, 2]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 324\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print len(d)\n",
    "x, y = d[10:20], d[11:21]\n",
    "print [np.argmax(j) for j in x]\n",
    "print [np.argmax(j) for j in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dirrs = ['sentiment/train/', 'sentiment/test/']\n",
    "# sent = []\n",
    "# for dirr in dirrs:\n",
    "#     print dirr\n",
    "#     l = listdir(dirr+'pos')\n",
    "#     print 'pos'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'pos/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))\n",
    "#     l = listdir(dirr+'neg')\n",
    "#     print 'neg'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'neg/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# m = Word2Vec.load('embedding.model')\n",
    "# data = []\n",
    "# for s in sent:\n",
    "#     a = [m[x] for x in s]\n",
    "#     data.append(a)\n",
    "# data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def sam(d):\n",
    "#     r = randint(0, data.shape[0] - 1)\n",
    "#     t = d[r]\n",
    "#     x = t[:-1]\n",
    "#     y = t[1:]\n",
    "#     return np.array(x), np.array(y)\n",
    "\n",
    "# s = sample(data)\n",
    "# s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 60000\n",
    "hidden_layer = 128\n",
    "inp_out_size = 324\n",
    "learning_rate = 0.001\n",
    "num_steps = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.ops.variables.Variable object at 0x7fec999f1550>, <tensorflow.python.ops.variables.Variable object at 0x7fec9991e810>, <tensorflow.python.ops.variables.Variable object at 0x7fec99d18310>, <tensorflow.python.ops.variables.Variable object at 0x7fec998bfcd0>, <tensorflow.python.ops.variables.Variable object at 0x7fec998904d0>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Wxh = tf.Variable([[.1, .2], [.3, .4], [.5, .6], [.7, .8]], dtype=tf.float32)\n",
    "# Whh = tf.Variable([[.1, .2], [.3, .4]], dtype=tf.float32)\n",
    "# Why = tf.Variable([[.1, .2, .3, .4], [.4, .5, .6, .7]], dtype=tf.float32)\n",
    "Wxh = tf.Variable(tf.random_normal([inp_out_size, hidden_layer], mean=0, stddev=0.001))\n",
    "Whh = tf.Variable(tf.random_normal([hidden_layer, hidden_layer], mean=0, stddev=0.001))\n",
    "Why = tf.Variable(tf.random_normal([hidden_layer, inp_out_size], mean=0, stddev=0.001))\n",
    "bh = tf.Variable(tf.zeros([hidden_layer]), dtype=tf.float32)\n",
    "by = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "print tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    h = tf.tanh((tf.matmul(p, Whh))  + tf.matmul(i, Wxh) + bh)\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b*tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)\n",
    "\n",
    "# optimize_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x, _ = data.sample(d, 1)\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(n):\n",
    "        o, h = sess.run([outputs, states], {a:x, initial: h})\n",
    "        h = h.reshape(hidden_layer)\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * inp_out_size\n",
    "        x[o] = 1\n",
    "#         print np.argmax(x)\n",
    "        x = [x]\n",
    "    print ' '.join(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "ix = 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size)*num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "epoch 0, loss = 62.3323005075\n",
      "epoch 1000, loss = 61.7424958785\n",
      "epoch 2000, loss = 61.0934833974\n",
      "epoch 3000, loss = 60.364991567\n",
      "epoch 4000, loss = 59.4789452272\n",
      "epoch 5000, loss = 58.4058847976\n",
      "epoch 6000, loss = 57.1217561315\n",
      "epoch 7000, loss = 55.7063488599\n",
      "epoch 8000, loss = 54.1506430167\n",
      "epoch 9000, loss = 52.4346930967\n",
      "epoch 10000, loss = 50.5754941645\n",
      "epoch 11000, loss = 48.6238826605\n",
      "epoch 12000, loss = 46.6355078225\n",
      "epoch 13000, loss = 44.6707594997\n",
      "epoch 14000, loss = 42.6984123343\n",
      "epoch 15000, loss = 40.6581201184\n",
      "epoch 16000, loss = 38.5564589457\n",
      "epoch 17000, loss = 36.3937494854\n",
      "epoch 18000, loss = 34.2735126865\n",
      "epoch 19000, loss = 32.1144999519\n",
      "epoch 20000, loss = 30.0057051304\n",
      "epoch 21000, loss = 27.9036950753\n",
      "epoch 22000, loss = 25.9042972709\n",
      "epoch 23000, loss = 23.8681705416\n",
      "epoch 24000, loss = 21.9271990322\n",
      "epoch 25000, loss = 20.0826783981\n",
      "epoch 26000, loss = 18.3177408138\n",
      "epoch 27000, loss = 16.7317084733\n",
      "epoch 28000, loss = 15.2434894312\n",
      "epoch 29000, loss = 13.907385802\n",
      "epoch 30000, loss = 12.6806794219\n",
      "epoch 31000, loss = 11.591871809\n",
      "epoch 32000, loss = 10.6449557099\n",
      "epoch 33000, loss = 9.800786814\n",
      "epoch 34000, loss = 9.06076515201\n",
      "epoch 35000, loss = 8.40285661436\n",
      "epoch 36000, loss = 7.83851540682\n",
      "epoch 37000, loss = 7.31348615507\n",
      "epoch 38000, loss = 6.84887537605\n",
      "epoch 39000, loss = 6.44629364714\n",
      "epoch 40000, loss = 6.081780764\n",
      "epoch 41000, loss = 5.75013797255\n",
      "epoch 42000, loss = 5.4620225997\n",
      "epoch 43000, loss = 5.19265868649\n",
      "epoch 44000, loss = 4.94954003635\n",
      "epoch 45000, loss = 4.72133785165\n",
      "epoch 46000, loss = 4.51703927478\n",
      "epoch 47000, loss = 4.32543546439\n",
      "epoch 48000, loss = 4.1515319934\n",
      "epoch 49000, loss = 3.99107659397\n",
      "epoch 50000, loss = 3.84620160707\n",
      "epoch 51000, loss = 3.70112461903\n",
      "epoch 52000, loss = 3.57564039778\n",
      "epoch 53000, loss = 3.45762141572\n",
      "epoch 54000, loss = 3.34290065983\n",
      "epoch 55000, loss = 3.23807646178\n",
      "epoch 56000, loss = 3.14016570695\n",
      "epoch 57000, loss = 3.04986602162\n",
      "epoch 58000, loss = 2.95482349793\n",
      "epoch 59000, loss = 2.87360418295\n"
     ]
    }
   ],
   "source": [
    "print epochs\n",
    "h = np.zeros(hidden_layer)\n",
    "for i in range(epochs):\n",
    "#     x, y = data.sample(d, num_steps)\n",
    "    if ix + num_steps >= len(d):\n",
    "        ix = 0\n",
    "        \n",
    "    h = np.zeros(hidden_layer)\n",
    "    x, y = d[ix : ix + num_steps], d[ix + 1 : ix + num_steps + 1]   \n",
    "    l, h, _ = sess.run([loss, states, optimize_op], {a: x, b: y, initial: h}) \n",
    "    smooth_loss = smooth_loss * 0.999 + l * 0.001\n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch {0}, loss = {1}'.format(i, smooth_loss)\n",
    "#         print sess.run(Whh)[0][:30]\n",
    "    ix += num_steps\n",
    "        \n",
    "#         generate(sess,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antheming the mist , her shoon \n",
      " Fancy 's the eye \n",
      " One lip is at home : \n",
      " Cloys will Even from with Doth ! \n",
      " Spirit Jove her reaped , . \n",
      " She 's the cheek that however blue fade ; \n",
      " And queen doth with these pleasures ; \n",
      " Spirit of grew reaped corn ; \n",
      " Sweet 's the voice that however ; \n",
      " ! \n",
      " rooks 'll with busy caw , \n",
      " Foraging for sticks , : : \n",
      " The shalt faggot Beauties same , and busy caw , \n",
      " Hatching caw her \n",
      " \n",
      " She ? her Fancy , a , \n",
      " Blushing through does at sing shoon \n",
      " \n",
      " Oh joys as spoilt soft , \n",
      " Blushing a touch at with busy caw up \n",
      " Like three her \n",
      " \n",
      " Sweet , the Beauties that doth earth thy pelteth \n",
      " \n",
      " She will mix these Quiet on her \n",
      " When the bee-hive casts sing Spring \n",
      " Fades blue thou and bells to May \n",
      " Pleasure never see Fancy with \n",
      " Quiet she her mossy Quiet , her mossy Where ; \n",
      " Every thing , spoilt by such joys are spoilt does , \n",
      " busy caw her \n",
      " Doth place , \n",
      " While to bubbles when rain . \n",
      " \n",
      " Oh , faggot to thy mind \n",
      " Doth ! let sweet Hatching feet her \n",
      " \n",
      " for see and the every fade , \n",
      " And such a spoilt hath every ; \n",
      " When the thought still its shuffled \n",
      " \n",
      " Oh see alway : And let hear are bells from \n",
      " Cloys \n",
      " Meagre 'll with busy caw ? \n",
      " Like three high-commission prison-string : —send then , hear Beauties does , and \n",
      " ! \n",
      " Meagre sweet Pleasure melteth \n",
      " Spirit of grew reaped corn ; \n",
      " Sweet hyacinth , alway how Sapphire queen doth hath these caw , \n",
      " Spirit of a winter 's night ; \n",
      " All the heaped still its swarm ; \n",
      " Acorns will faggot Fancy April mind \n",
      " \n",
      " would see Fancy with sweet rest \n",
      " Quiet soar \n",
      " soar loose her maid \n",
      " O sweet is conspiracy \n",
      " To \n",
      " Meagre from the self-same shower ; \n",
      " Then let winged Fancy \n",
      " \n",
      " Oh a spoilt hath goblet , \n",
      " And such joys as spoilt by ; \n",
      " ! will bring from with lark \n",
      " Quiet she her her loose ; \n",
      " Every thou shalt spoilt to chide \n",
      " bells lip May from \n",
      " Cloys with grew from her ; \n",
      " Then let winged Fancy \n",
      " Fades thing joys as spoilt by use \n",
      " And will mind these \n",
      " Rustle on her mossy nest , \n",
      " Fancy thee there , and send abroad , \n",
      " All queen , her ! ! \n",
      " Torment she she 'll bring.— \n",
      " She has see mossy corn ; \n",
      " Then hyacinth her prison-string \n",
      " And such a spoilt by by , \n",
      " And queen touch earth with busy caw her \n",
      " \n",
      " Like\n"
     ]
    }
   ],
   "source": [
    "generate(sess, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13754342,  0.19704947, -0.05745095, ...,  0.16697039,\n",
       "        -0.08112098, -0.0941937 ],\n",
       "       [-0.03235122, -0.04876881,  0.05504122, ..., -0.26356348,\n",
       "        -0.08242013, -0.02991492],\n",
       "       [ 0.10348038, -0.13705294,  0.08280424, ...,  0.24422346,\n",
       "        -0.07879512,  0.01655063],\n",
       "       ..., \n",
       "       [ 0.12828396, -0.08228143,  0.23148909, ...,  0.19258361,\n",
       "         0.03882705,  0.0760185 ],\n",
       "       [ 0.17688558,  0.07320131,  0.29498637, ...,  0.09559939,\n",
       "         0.13998733, -0.15826966],\n",
       "       [-0.21004789,  0.13676059, -0.11080284, ..., -0.07399724,\n",
       "        -0.10957402,  0.20072849]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
