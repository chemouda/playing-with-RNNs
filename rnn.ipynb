{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from random import randint\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[308, 15, 3, 307, 298]\n",
      "[15, 3, 307, 298, 168]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 324\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "x, y = data.sample(d, 5)\n",
    "print x[:2]\n",
    "print [np.argmax(j) for j in x]\n",
    "print [np.argmax(j) for j in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirrs = ['sentiment/train/', 'sentiment/test/']\n",
    "sent = []\n",
    "for dirr in dirrs:\n",
    "    print dirr\n",
    "    l = listdir(dirr+'pos')\n",
    "    print 'pos'\n",
    "    for r in l:\n",
    "        t = codecs.open(dirr+'pos/'+r,'r',encoding='utf8').read()\n",
    "        sent.append(nltk.word_tokenize(t))\n",
    "    l = listdir(dirr+'neg')\n",
    "    print 'neg'\n",
    "    for r in l:\n",
    "        t = codecs.open(dirr+'neg/'+r,'r',encoding='utf8').read()\n",
    "        sent.append(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Word2Vec.load('embedding.model')\n",
    "data = []\n",
    "for s in sent:\n",
    "    a = [m[x] for x in s]\n",
    "    data.append(a)\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sam(d):\n",
    "    r = randint(0, data.shape[0] - 1)\n",
    "    t = d[r]\n",
    "    x = t[:-1]\n",
    "    y = t[1:]\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "s = sample(data)\n",
    "s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 20000\n",
    "hidden_layer = 50\n",
    "inp_out_size = 324\n",
    "learning_rate = 0.1\n",
    "num_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wxh = tf.Variable([[.1, .2], [.3, .4], [.5, .6], [.7, .8]], dtype=tf.float32)\n",
    "# Whh = tf.Variable([[.1, .2], [.3, .4]], dtype=tf.float32)\n",
    "# Why = tf.Variable([[.1, .2, .3, .4], [.4, .5, .6, .7]], dtype=tf.float32)\n",
    "Wxh = tf.Variable(tf.random_normal([inp_out_size, hidden_layer], mean=0, stddev=0.001))\n",
    "Whh = tf.Variable(tf.random_normal([hidden_layer, hidden_layer], mean=0, stddev=0.001))\n",
    "Why = tf.Variable(tf.ones([hidden_layer, inp_out_size]))\n",
    "bb = tf.zeros([hidden_layer], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "#     h = tf.nn.tanh(tf.squeeze(tf.matmul(i, Wxh)) + tf.squeeze(tf.matmul(p, Whh)) + bb)\n",
    "    h = tf.tanh( tf.matmul(p, Whh) + tf.matmul(i, Wxh) + bb )\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    print h\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"scan_1/while/Reshape_2:0\", shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.matmul(states, Why)\n",
    "output_split = tf.split(0, num_steps, outputs)\n",
    "target_split = tf.split(0, num_steps, b)\n",
    "loss = tf.add_n([tf.nn.softmax_cross_entropy_with_logits(output_split[j], target_split[j]) for j in range(num_steps - 1, -1, -1)])\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "trainables = tf.trainable_variables()\n",
    "grads = tf.gradients(loss, trainables)\n",
    "grads, _ = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "grad_var_pairs = zip(grads, trainables)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "optimize_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x, _ = data.sample(d, 1)\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    for i in range(n):\n",
    "        o = sess.run(outputs, {a:x, initial: np.zeros(hidden_layer)})\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * inp_out_size\n",
    "        x[o] = 1\n",
    "        x = [x]\n",
    "    print gen\n",
    "    print ' '.join(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "epoch 0, loss = [ 57.80743408]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "epoch 3000, loss = [ 47.09765244]\n",
      "[1, 2, 1, 1, 1, 1, 11, 3, 3, 3]\n",
      "epoch 6000, loss = [ 38.35453415]\n",
      "[1, 6, 4, 1, 11, 3, 3, 38, 2, 6]\n",
      "epoch 9000, loss = [ 21.31944656]\n",
      "[5, 3, 4, 1, 26, 3, 315, 47, 15, 155]\n",
      "epoch 12000, loss = [ 17.59990501]\n",
      "[13, 6, 260, 2, 1, 40, 3, 271, 3, 102]\n",
      "epoch 15000, loss = [ 11.21655083]\n",
      "[11, 33, 136, 3, 224, 8, 1, 11, 2, 23]\n",
      "epoch 18000, loss = [ 5.99588251]\n",
      "[1, 12, 3, 266, 2, 27, 101, 249, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print epochs\n",
    "for i in range(epochs):\n",
    "    x, y = data.sample(d, num_steps)\n",
    "    h = np.zeros(hidden_layer)\n",
    "    l, o, _ = sess.run([loss, outputs, optimize_op], {a: x, b: y, initial: h})\n",
    "    if i % 3000 == 0:\n",
    "        print 'epoch {0}, loss = {1}'.format(i, l)\n",
    "        print [np.argmax(j) for j in o]\n",
    "#             t = [id_to_token[np.argmax(j)] for j in o]\n",
    "#             print t\n",
    "\n",
    "#     x, y = data.sample(d, 10)\n",
    "#     o, s = sess.run([outputs, states], {a: x, b: y})\n",
    "#     print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wealth', ',', '\\n', 'And', 'the', 'thought', 'still', ',', '\\n', 'And', 'the']\n",
      "wealth , \n",
      " And the thought still , \n",
      " And the\n"
     ]
    }
   ],
   "source": [
    "generate(sess, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.80843545e-02,  -2.23145187e-02,   2.55296737e-01, ...,\n",
       "          8.79745930e-02,  -9.53120664e-02,   5.36855757e-02],\n",
       "       [ -6.67928457e-02,   1.80839196e-01,  -3.88792939e-02, ...,\n",
       "         -7.11535895e-03,   3.62812132e-02,   1.02751710e-01],\n",
       "       [ -2.31132492e-01,   3.07729449e-02,  -3.23402435e-02, ...,\n",
       "         -3.96190584e-03,  -4.81384844e-02,  -3.99097987e-02],\n",
       "       ..., \n",
       "       [ -7.29469061e-02,   1.38710096e-01,  -4.88553494e-02, ...,\n",
       "          4.21650447e-02,  -2.06389315e-02,   4.26352471e-02],\n",
       "       [ -8.64108354e-02,   2.56500185e-01,   1.55067630e-02, ...,\n",
       "          8.46683085e-02,   1.25194490e-01,   1.00607742e-02],\n",
       "       [ -4.52823341e-02,  -6.01971708e-02,   8.82701799e-02, ...,\n",
       "         -1.96572095e-02,  -1.45239681e-02,   1.67555641e-04]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
