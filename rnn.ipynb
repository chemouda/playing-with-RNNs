{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from random import randint\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "735\n",
      "[29, 56, 8, 1, 82, 9, 45, 22, 24, 67]\n",
      "[56, 8, 1, 82, 9, 45, 22, 24, 67, 2]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 324\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print len(d)\n",
    "x, y = d[10:20], d[11:21]\n",
    "print [np.argmax(j) for j in x]\n",
    "print [np.argmax(j) for j in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dirrs = ['sentiment/train/', 'sentiment/test/']\n",
    "# sent = []\n",
    "# for dirr in dirrs:\n",
    "#     print dirr\n",
    "#     l = listdir(dirr+'pos')\n",
    "#     print 'pos'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'pos/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))\n",
    "#     l = listdir(dirr+'neg')\n",
    "#     print 'neg'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'neg/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# m = Word2Vec.load('embedding.model')\n",
    "# data = []\n",
    "# for s in sent:\n",
    "#     a = [m[x] for x in s]\n",
    "#     data.append(a)\n",
    "# data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def sam(d):\n",
    "#     r = randint(0, data.shape[0] - 1)\n",
    "#     t = d[r]\n",
    "#     x = t[:-1]\n",
    "#     y = t[1:]\n",
    "#     return np.array(x), np.array(y)\n",
    "\n",
    "# s = sample(data)\n",
    "# s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 10000\n",
    "hidden_layer = 128\n",
    "inp_out_size = 324\n",
    "learning_rate = 0.001\n",
    "num_steps = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wxh = tf.Variable([[.1, .2], [.3, .4], [.5, .6], [.7, .8]], dtype=tf.float32)\n",
    "# Whh = tf.Variable([[.1, .2], [.3, .4]], dtype=tf.float32)\n",
    "# Why = tf.Variable([[.1, .2, .3, .4], [.4, .5, .6, .7]], dtype=tf.float32)\n",
    "Wxh = tf.Variable(tf.random_normal([inp_out_size, hidden_layer], mean=0, stddev=0.001))\n",
    "Whh = tf.Variable(tf.random_normal([hidden_layer, hidden_layer], mean=0, stddev=0.001))\n",
    "Why = tf.Variable(tf.random_normal([hidden_layer, inp_out_size], mean=0, stddev=0.001))\n",
    "bh = tf.Variable(tf.zeros([hidden_layer]), dtype=tf.float32)\n",
    "by = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    h = tf.tanh((tf.matmul(p, Whh))  + tf.matmul(i, Wxh) + bh)\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b*tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "# trainable_variables = tf.trainable_variables()\n",
    "# grad_var_pairs = optimizer.compute_gradients(loss, trainable_variables)\n",
    "# print grad_var_pairs[1]\n",
    "# clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "# optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)\n",
    "\n",
    "optimize_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x, _ = data.sample(d, 1)\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(n):\n",
    "        o, h = sess.run([outputs, states], {a:x, initial: h})\n",
    "        h = h.reshape(hidden_layer)\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * inp_out_size\n",
    "        x[o] = 1\n",
    "#         print np.argmax(x)\n",
    "        x = [x]\n",
    "    print ' '.join(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "ix = 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size)*num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "epoch 0, loss = 8.43297839833\n",
      "epoch 1000, loss = 7.87199647324\n",
      "epoch 2000, loss = 7.38318113127\n",
      "epoch 3000, loss = 6.92831863497\n",
      "epoch 4000, loss = 8.80318795135\n",
      "epoch 5000, loss = 8.47478664454\n",
      "epoch 6000, loss = 6.78799833011\n",
      "epoch 7000, loss = 5.92561129611\n",
      "epoch 8000, loss = 5.41741338838\n",
      "epoch 9000, loss = 5.08114284208\n"
     ]
    }
   ],
   "source": [
    "print epochs\n",
    "h = np.zeros(hidden_layer)\n",
    "for i in range(epochs):\n",
    "#     x, y = data.sample(d, num_steps)\n",
    "    if ix + num_steps >= len(d):\n",
    "        ix = 0\n",
    "        \n",
    "    h = np.zeros(hidden_layer)\n",
    "    x, y = d[ix : ix + num_steps], d[ix + 1 : ix + num_steps + 1]   \n",
    "    l, h, _ = sess.run([loss, states, optimize_op], {a: x, b: y, initial: h}) \n",
    "    smooth_loss = smooth_loss * 0.999 + l * 0.001\n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch {0}, loss = {1}'.format(i, smooth_loss)\n",
    "#         print sess.run(Whh)[0][:30]\n",
    "    ix += num_steps\n",
    "        \n",
    "#         generate(sess,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " She will mix these pleasures up \n",
      " Like three fit wines in a : \n",
      " Let would faggot earth thy these \n",
      " Quiet on her mossy nest ; 's Fancy all fade shalt \n",
      " lip queen , \n",
      " From dewy ploughboy and morn feet , \n",
      " While shalt joys the goblet pelteth . \n",
      " Let would faggot earth rest these \n",
      " Quiet on the mossy skin ; \n",
      " Summer the joys as winter-thin shuffled chide , \n",
      " All queen faggot mind these caw \n",
      " Quiet the the mossy \n",
      " And , joys : and by use , \n",
      " All through faggot mind From \n",
      " Quiet the morn or thorny swarm ; \n",
      " All ripe heaped alway \n",
      " Sapphire queen and these shower Quiet , her loose ; \n",
      " While the joys as winter-thin shuffled chide \n",
      " Let would heaped earth rest these \n",
      " Quiet on the mossy skin ; \n",
      " Pleasure as is all home : \n",
      " Blushing through the mind these From , \n",
      " antheming the morn ? \n",
      " And , in the same moment , \n",
      " She will God earth alarm these \n",
      " \n",
      " Quiet the morn : \n",
      " And every leaf , and every flower \n",
      " \n",
      " lip queen 'll dart these caw the morn and nest , the face \n",
      " Acorns hyacinth , alway \n",
      " She she 'll these shower her , \n",
      " Sit 'd bubbles wines in a : \n",
      " Let would faggot earth rest these \n",
      " \n",
      " Quiet the morn : \n",
      " And every leaf , and every flower \n",
      " \n",
      " lip queen 'll dart these shower her bring.— \n",
      " Open wide the mind 's cage-door , \n",
      " shalt that the mind these From these 'll 'll bring.— shower her ? \n",
      " And , in the same : \n",
      " \n",
      " The queen and From these 'll and bring.— feet , \n",
      " Where 's the cheek , doth not fade , \n",
      " All queen faggot mind these \n",
      " Quiet the morn casts its swarm ; \n",
      " Acorns ripe Hebe alway \n",
      " Sapphire queen leash these \n",
      " Quiet on her mossy nest ; \n",
      " Then the hurry doth wing of May \n",
      " \n",
      " ! queen 'll these pleasures up \n",
      " In a dark and \n",
      " To , \n",
      " Or the God sweet Meagre these leash her zone \n",
      " In the winged Fancy roam , \n",
      " Pleasure sweet Hebe 's bells of May mind \n",
      " From dewy ploughboy 's her : \n",
      " Where 's the voice , however soft , \n",
      " Meagre these leash 'll bring.— zone cloudward soar winged Fancy wander 's joys as is shalt by use , \n",
      " \n",
      " Sapphire queen 'll these shower caw , \n",
      " And 'd there , and by : \n",
      " \n",
      " through queen and From these Quiet \n",
      " Quiet the her mossy \n",
      " And , in : and by use , \n",
      " All through God mind From \n",
      " Quiet the morn or thorny swarm ; \n",
      " Acorns ripe Hebe alway \n",
      " Sapphire queen leash these \n",
      " Quiet on her\n"
     ]
    }
   ],
   "source": [
    "generate(sess, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13754342,  0.19704947, -0.05745095, ...,  0.16697039,\n",
       "        -0.08112098, -0.0941937 ],\n",
       "       [-0.03235122, -0.04876881,  0.05504122, ..., -0.26356348,\n",
       "        -0.08242013, -0.02991492],\n",
       "       [ 0.10348038, -0.13705294,  0.08280424, ...,  0.24422346,\n",
       "        -0.07879512,  0.01655063],\n",
       "       ..., \n",
       "       [ 0.12828396, -0.08228143,  0.23148909, ...,  0.19258361,\n",
       "         0.03882705,  0.0760185 ],\n",
       "       [ 0.17688558,  0.07320131,  0.29498637, ...,  0.09559939,\n",
       "         0.13998733, -0.15826966],\n",
       "       [-0.21004789,  0.13676059, -0.11080284, ..., -0.07399724,\n",
       "        -0.10957402,  0.20072849]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
