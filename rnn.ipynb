{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from random import randint\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1909\n",
      "12063\n",
      "[7, 163, 68, 81, 396, 21, 14, 44, 62, 78]\n",
      "[163, 68, 81, 396, 21, 14, 44, 62, 78, 10]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1500\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print len(d)\n",
    "x, y = d[10:20], d[11:21]\n",
    "print [np.argmax(j) for j in x]\n",
    "print [np.argmax(j) for j in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dirrs = ['sentiment/train/', 'sentiment/test/']\n",
    "# sent = []\n",
    "# for dirr in dirrs:\n",
    "#     print dirr\n",
    "#     l = listdir(dirr+'pos')\n",
    "#     print 'pos'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'pos/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))\n",
    "#     l = listdir(dirr+'neg')\n",
    "#     print 'neg'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'neg/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# m = Word2Vec.load('embedding.model')\n",
    "# data = []\n",
    "# for s in sent:\n",
    "#     a = [m[x] for x in s]\n",
    "#     data.append(a)\n",
    "# data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def sam(d):\n",
    "#     r = randint(0, data.shape[0] - 1)\n",
    "#     t = d[r]\n",
    "#     x = t[:-1]\n",
    "#     y = t[1:]\n",
    "#     return np.array(x), np.array(y)\n",
    "\n",
    "# s = sample(data)\n",
    "# s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 10000\n",
    "hidden_layer = 128\n",
    "inp_out_size = 1500\n",
    "learning_rate = 0.1\n",
    "num_steps = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.ops.variables.Variable object at 0x7f8b8a08d2d0>, <tensorflow.python.ops.variables.Variable object at 0x7f8b89fe4850>, <tensorflow.python.ops.variables.Variable object at 0x7f8b89f5c450>, <tensorflow.python.ops.variables.Variable object at 0x7f8b8a00aa90>, <tensorflow.python.ops.variables.Variable object at 0x7f8b8a00aad0>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Wxh = tf.Variable([[.1, .2], [.3, .4], [.5, .6], [.7, .8]], dtype=tf.float32)\n",
    "# Whh = tf.Variable([[.1, .2], [.3, .4]], dtype=tf.float32)\n",
    "# Why = tf.Variable([[.1, .2, .3, .4], [.4, .5, .6, .7]], dtype=tf.float32)\n",
    "Wxh = tf.Variable(tf.random_normal([inp_out_size, hidden_layer], mean=0, stddev=0.001))\n",
    "Whh = tf.Variable(tf.random_normal([hidden_layer, hidden_layer], mean=0, stddev=0.001))\n",
    "Why = tf.Variable(tf.random_normal([hidden_layer, inp_out_size], mean=0, stddev=0.001))\n",
    "bh = tf.Variable(tf.zeros([hidden_layer]), dtype=tf.float32)\n",
    "by = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "print tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    h = tf.tanh((tf.matmul(p, Whh))  + tf.matmul(i, Wxh) + bh)\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b*tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)\n",
    "\n",
    "# optimize_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x, _ = data.sample(d, 1)\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(n):\n",
    "        o, h = sess.run([outputs, states], {a:x, initial: h})\n",
    "        h = h.reshape(hidden_layer)\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * inp_out_size\n",
    "        x[o] = 1\n",
    "#         print np.argmax(x)\n",
    "        x = [x]\n",
    "    print ' '.join(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "ix = 0\n",
    "smooth_loss = -np.log(1.0 / vocab_size)*num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if already trained previously, just restore\n",
    "to_restore = True\n",
    "\n",
    "if to_restore:\n",
    "    saver.restore(sess, 'model.ckpt')\n",
    "else:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "epoch 0, loss = 15.9441945378\n",
      "epoch 1000, loss = 12.7052840564\n",
      "epoch 2000, loss = 11.2963803434\n",
      "epoch 3000, loss = 10.468597103\n",
      "epoch 4000, loss = 10.0458583511\n",
      "epoch 5000, loss = 9.87956408773\n",
      "epoch 6000, loss = 9.83136852966\n",
      "epoch 7000, loss = 9.53692478479\n",
      "epoch 8000, loss = 9.21046329804\n",
      "epoch 9000, loss = 9.04049821997\n"
     ]
    }
   ],
   "source": [
    "print epochs\n",
    "h = np.zeros(hidden_layer)\n",
    "for i in range(epochs):\n",
    "#     x, y = data.sample(d, num_steps)\n",
    "    if ix + num_steps >= len(d):\n",
    "        ix = 0\n",
    "    num_steps = num_steps - 1 if i % 2 == 0 else num_steps + 1 # variable length sequences\n",
    "    h = np.zeros(hidden_layer)\n",
    "    x, y = d[ix : ix + num_steps], d[ix + 1 : ix + num_steps + 1]   \n",
    "    l, h, _ = sess.run([loss, states, optimize_op], {a: x, b: y, initial: h}) \n",
    "    smooth_loss = smooth_loss * 0.999 + l * 0.001\n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch {0}, loss = {1}'.format(i, smooth_loss)\n",
    "#         print sess.run(Whh)[0][:30]\n",
    "    ix += num_steps\n",
    "        \n",
    "#         generate(sess,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.ckpt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving model\n",
    "saver.save(sess, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrorists and I said , we are n't unk unk the place . They are n't them paying the most place . \n",
      " \n",
      " The country ; you have a big where they like this , I 'm going to say but other countries , unk do not going to unk our country . unk you 've him that look at how to have a very important election . \n",
      " \n",
      " You know I said oh we 're just â€” and the deal . I know many of it all over the way , but this country ; we got it . And you are n't the other day we can see so many of that and we do n't know what we are doing . They 're building the one that statement ; that got some heard of our country . And you ca n't have unk that but what are doing ? I like the other day . I mean you all the unk election in -- it 's something to unk . It 's n't unk to them . It was just them in Iraq . So we have been selling this other effort ago , I 'm a unk election coming up what can know and oh talk come in unk unk airport and you can do n't us ; you are some of them . \n",
      " \n",
      " The country 's not . So they do the right over but something we 're just a unk . It 's n't unk -- the place are the World day for have a very successful guy . But we do n't they have a lot of other reasons in serious people . I mean the Euro , China is going to be the border ; you know a unk election coming up . I 'm got to fall . There are it to unk . It 's South Korea , but of this thing . They do n't know years ago , I said oh unk ; they are we doing ; I like them and I speak to unk the most people . \n",
      " \n",
      " We spent $ 2 trillion in Iraq , but 're just going to get something in . I think do it ; that election this . They 're going to say you know what we are n't have done just a unk on them is doing ago , they 're going to have the Middle East , they Afghanistan . You ca n't the worst unk , or that . So we have to -- money . I 'm not a hole that you 've believe paying in a lot of his reasons . And I have a hole election ; we are doing people with our country allies , I mean you know what we have a unk where they have a very incredible people to power in the right . So we got a big things . \n",
      " \n",
      " We have\n"
     ]
    }
   ],
   "source": [
    "generate(sess, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03073823,  0.16024236,  0.16885728, ...,  0.05288818,\n",
       "        -0.07688183, -0.03028169],\n",
       "       [-0.24785835, -0.04390815,  0.1450949 , ..., -0.05311445,\n",
       "        -0.01250314,  0.11281139],\n",
       "       [-0.33347312, -0.20284222,  0.23098749, ..., -0.10504574,\n",
       "        -0.28402022,  0.03956333],\n",
       "       ..., \n",
       "       [ 0.05662825,  0.16807128,  0.00710124, ...,  0.22454964,\n",
       "        -0.06467728,  0.10287257],\n",
       "       [-0.06466194, -0.10157995, -0.11470366, ...,  0.09235065,\n",
       "         0.3563292 ,  0.02229096],\n",
       "       [ 0.08362083,  0.13572359,  0.1389745 , ...,  0.22135787,\n",
       "         0.00691832,  0.11428044]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(Whh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
