{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "from os import listdir\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from random import randint\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659\n",
      "12794\n",
      "[5, 1, 11, 25, 22, 1196, 5, 1, 11, 25]\n",
      "[1, 11, 25, 22, 1196, 5, 1, 11, 25, 22]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1659\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print len(d)\n",
    "x, y = d[10:20], d[11:21]\n",
    "print [np.argmax(j) for j in x]\n",
    "print [np.argmax(j) for j in y] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dirrs = ['sentiment/train/', 'sentiment/test/']\n",
    "# sent = []\n",
    "# for dirr in dirrs:\n",
    "#     print dirr\n",
    "#     l = listdir(dirr+'pos')\n",
    "#     print 'pos'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'pos/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))\n",
    "#     l = listdir(dirr+'neg')\n",
    "#     print 'neg'\n",
    "#     for r in l:\n",
    "#         t = codecs.open(dirr+'neg/'+r,'r',encoding='utf8').read()\n",
    "#         sent.append(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# m = Word2Vec.load('embedding.model')\n",
    "# data = []\n",
    "# for s in sent:\n",
    "#     a = [m[x] for x in s]\n",
    "#     data.append(a)\n",
    "# data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def sam(d):\n",
    "#     r = randint(0, data.shape[0] - 1)\n",
    "#     t = d[r]\n",
    "#     x = t[:-1]\n",
    "#     y = t[1:]\n",
    "#     return np.array(x), np.array(y)\n",
    "\n",
    "# s = sample(data)\n",
    "# s[0].shape, s[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 16000\n",
    "hidden_layer = 128\n",
    "inp_out_size = 1659\n",
    "learning_rate = 0.1\n",
    "num_steps = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wxh = tf.Variable([[.1, .2], [.3, .4], [.5, .6], [.7, .8]], dtype=tf.float32)\n",
    "# Whh = tf.Variable([[.1, .2], [.3, .4]], dtype=tf.float32)\n",
    "# Why = tf.Variable([[.1, .2, .3, .4], [.4, .5, .6, .7]], dtype=tf.float32)\n",
    "Wxh = tf.Variable(tf.random_normal([inp_out_size, hidden_layer], mean=0, stddev=0.001))\n",
    "Whh = tf.Variable(tf.random_normal([hidden_layer, hidden_layer], mean=0, stddev=0.001))\n",
    "Why = tf.Variable(tf.random_normal([hidden_layer, inp_out_size], mean=0, stddev=0.001))\n",
    "bh = tf.Variable(tf.zeros([hidden_layer]), dtype=tf.float32)\n",
    "by = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    h = tf.tanh((tf.matmul(p, Whh))  + tf.matmul(i, Wxh) + bh)\n",
    "    o = tf.nn.softmax(tf.matmul(h, Why) + by)\n",
    "    o = tf.reshape(o, [vocab_size])\n",
    "    print o\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    print h\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"scan/while/Reshape_2:0\", shape=(1659,), dtype=float32)\n",
      "Tensor(\"scan/while/Reshape_3:0\", shape=(128,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b*tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.AdagradOptimizer(0.1)\n",
    "optimize_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(sess, n):\n",
    "    x, _ = data.sample(d, 1)\n",
    "    gen = [id_to_token[np.argmax(x[0])]]\n",
    "    h = np.zeros(hidden_layer)\n",
    "    for i in range(n):\n",
    "        o, h = sess.run([outputs, states], {a:x, initial: h})\n",
    "        h = h.reshape(hidden_layer)\n",
    "        o = np.argmax(o[0])\n",
    "        gen.append(id_to_token[o])\n",
    "        x = [0] * inp_out_size\n",
    "        x[o] = 1\n",
    "#         print np.argmax(x)\n",
    "        x = [x]\n",
    "    print ' '.join(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "epoch 0, loss = 6.63712739944\n",
      "epoch 1000, loss = 13.0196733475\n",
      "epoch 2000, loss = 9.66602134705\n",
      "epoch 3000, loss = 7.41388654709\n",
      "epoch 4000, loss = 6.9941072464\n",
      "epoch 5000, loss = 5.35630130768\n",
      "epoch 6000, loss = 6.6308889389\n",
      "epoch 7000, loss = 7.49393463135\n",
      "epoch 8000, loss = 7.3836274147\n",
      "epoch 9000, loss = 6.83894872665\n",
      "epoch 10000, loss = 4.13149738312\n",
      "epoch 11000, loss = 4.32619714737\n",
      "epoch 12000, loss = 4.84167432785\n",
      "epoch 13000, loss = 0.269762933254\n",
      "epoch 14000, loss = 11.6245145798\n",
      "epoch 15000, loss = 1.87798666954\n"
     ]
    }
   ],
   "source": [
    "print epochs\n",
    "for i in range(epochs):\n",
    "#     x, y = data.sample(d, num_steps)\n",
    "    if ix + num_steps > len(d):\n",
    "        ix = 0\n",
    "    x, y = d[ix : ix + num_steps], d[ix + 1 : ix + num_steps + 1]\n",
    "    \n",
    "    h = np.zeros(hidden_layer)\n",
    "    l, o, _ = sess.run([loss, states, optimize_op], {a: x, b: y, initial: h})\n",
    "    if i % 1000 == 0:\n",
    "        print 'epoch {0}, loss = {1}'.format(i, l)\n",
    "    ix += num_steps\n",
    "        \n",
    "#         generate(sess,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " if ( clone_flags & CLONE_VFORK ) ; \n",
      " return ERR_PTR ( unsigned long clone_flags , \n",
      " unsigned long , now we \n",
      " * can unshare are handling the process it */ \n",
      " * CLONE_SYSVSEM , \n",
      " * . \n",
      " if ( fs , tmp copy ) ; \n",
      " INIT_LIST_HEAD ( & p- > thread_group , \n",
      " sig- > oom_score_adj = 0 ; \n",
      " # endif \n",
      " \n",
      " /* \n",
      " * If is threads to a check the we it the we can */ \n",
      " if ( pid , \n",
      " int __user data_vm ) ; \n",
      " \n",
      " if ( clone_flags & CLONE_VFORK ) ) ; \n",
      " if ( retval = 0 ; \n",
      " \n",
      " if ( clone_flags & CLONE_VFORK ) { \n",
      " # ifdef CONFIG_FUTEX \n",
      " # endif \n",
      " \n",
      " # ifdef CONFIG_DEBUG_MUTEXES \n",
      " unsigned long , NULL ( unshare_flags , p ) ; \n",
      " } \n",
      " \n",
      " /* \n",
      " * Siblings of \n",
      " * the \n",
      " * this do and session . \n",
      " * is 's if do_fork stack_size \n",
      " * child \n",
      " * for a lock the \n",
      " \n",
      " if ( clone_flags & CLONE_VFORK ) \n",
      " # endif \n",
      " \n",
      " # ifdef CONFIG_DEBUG_MUTEXES \n",
      " unsigned long unshare_flags ) ) ; \n",
      " \n",
      " atomic_set ( & p- > thread_group , 1 ) ; \n",
      " } \n",
      " \n",
      " retval = -ENOMEM ; \n",
      " struct pid ! ( ( clone_flags & ( ) ) ; \n",
      " } \n",
      " \n",
      " # ifdef CONFIG_PROVE_RCU \n",
      " tsk- > fs ; \n",
      " } \n",
      " \n",
      " /* \n",
      " * = ! if ( unsigned ) ; \n",
      " } \n",
      " \n",
      " /* \n",
      " * = PTRACE_EVENT_FORK . \n",
      " * \n",
      " * of the the group . parent to of the need to if the signal \n",
      " * called from the \n",
      " */ \n",
      " if ( unshare_flags & CLONE_THREAD ) { \n",
      " if ( unshare_flags ) ; \n",
      " get_task_struct ( p ) ; \n",
      " if ( ! & p- > i = 0 ; \n",
      " # endif \n",
      " \n",
      " return p ; \n",
      " } \n",
      " \n",
      " /* \n",
      " * Siblings be handling , stack_size , \n",
      " int __user * , mm ) \n",
      " { \n",
      " struct fs_struct vfork = current- > \n",
      " * 0 ; \n",
      " \n",
      " if ( unsigned long , process_counts ) ; \n",
      " } \n",
      " \n",
      " void = ( struct file ! ) \n",
      " { \n",
      " unsigned long ; \n",
      " struct mm_struct *p ) \n",
      " { \n",
      " SYSCALL_DEFINE1 ( clone_flags , pid ) ) { \n",
      " /* \n",
      " * Not implemented of the task , be int the new_fd , as mm_struct signal ) , PIDTYPE_PID , 0 , oldmm ) \n",
      " # endif \n",
      " \n",
      " # ifndef MAX_THREADS \n",
      " return -ENOMEM ; \n",
      " \n",
      " if ( clone_flags & CLONE_VFORK ) ; \n",
      " return 0 ; \n",
      " \n",
      " /* \n",
      " * , implemented , call a ) */ \n",
      " if ( retval ) \n",
      " goto bad_fork_cleanup_namespaces ; \n",
      " \n",
      " return 0 ; \n",
      " } \n",
      " \n",
      " static inline void free_thread_info ( struct task_struct *p )\n"
     ]
    }
   ],
   "source": [
    "generate(sess, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
