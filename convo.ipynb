{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1909\n",
      "12063\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1500\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print(len(d))\n",
    "x, y = d[10:20], d[11:21]\n",
    "max_word_length = max([len(id_to_token[np.argmax(j)]) for j in d])\n",
    "print(max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 15, 87, 1)\n"
     ]
    }
   ],
   "source": [
    "c_to_i, embedding = data.get_char_embedding()\n",
    "\n",
    "def word_generator(ix, num=10):\n",
    "    return d[ix : ix + num]\n",
    "\n",
    "def sequence_char_matrix(ix, num = 10):\n",
    "    # returns char matrices of all words in a sequence starting from ix and of length num    \n",
    "    words = [id_to_token[np.argmax(j)] for j in d[ix : ix + num]]\n",
    "    chars = [embedding[[c_to_i[c] for c in w]] for w in words]\n",
    "    to_pad = [max_word_length - t for t in map(lambda x: len(x), words)]\n",
    "    pad = [np.zeros([tp, embedding.shape[0]]) for tp in to_pad]\n",
    "    reshape = [1, max_word_length, embedding.shape[0], 1]\n",
    "    padded = np.concatenate([np.reshape(np.r_[ch, pd], reshape) for ch, pd in zip(chars, pad)], axis=0)\n",
    "    print(padded.shape)\n",
    "    return padded\n",
    "\n",
    "_ = sequence_char_matrix(3, 2)\n",
    "_ = word_generator(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 87\n"
     ]
    }
   ],
   "source": [
    "# CNN hyperparameters\n",
    "input_height = max_word_length\n",
    "input_width = embedding.shape[0]\n",
    "batch_size = 5\n",
    "print(input_height, input_width)\n",
    "filter_heights = [2, 3, 4]\n",
    "feature_maps = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(inp):\n",
    "    with tf.variable_scope('conv1') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[0], input_width, 1, feature_maps[0]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv1 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool1 = tf.squeeze(tf.nn.max_pool(conv1, ksize=[1, conv1.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[1], input_width, 1, feature_maps[1]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv2 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool2 = tf.squeeze(tf.nn.max_pool(conv2, ksize=[1, conv2.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    with tf.variable_scope('conv3') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[2], input_width, 1, feature_maps[2]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv3 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool3 = tf.squeeze(tf.nn.max_pool(conv3, ksize=[1, conv3.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    pool_total = tf.concat(0, [pool1, pool2, pool3], name='total_pool')\n",
    "    return pool_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_ = tf.placeholder(shape=[batch_size, max_word_length, embedding.shape[0], 1], dtype=tf.float32, name = 'cnn_in')\n",
    "words = tf.split(0, batch_size, input_)\n",
    "cnn_outputs = []\n",
    "with tf.variable_scope(\"CNN\") as scope:\n",
    "    for idx, word in enumerate(words):\n",
    "#         inp = tf.Variable(word, trainable=False, dtype=tf.float32)\n",
    "        if idx != 0:\n",
    "            scope.reuse_variables()\n",
    "        p = inference(word)\n",
    "        cnn_outputs.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN hyperparameters\n",
    "\n",
    "epochs = 5000\n",
    "hidden_layer = 128\n",
    "input_size = sum(feature_maps)\n",
    "output_size = vocab_size\n",
    "learning_rate = 0.1\n",
    "num_steps = 15\n",
    "inp_out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wxh = tf.Variable(tf.random_uniform([input_size, hidden_layer], minval=-0.05, maxval=0.05))\n",
    "Whh = tf.Variable(tf.random_uniform([hidden_layer, hidden_layer], minval=-0.05, maxval=0.05))\n",
    "Why = tf.Variable(tf.random_uniform([hidden_layer, output_size], minval=-0.05, maxval=0.05))\n",
    "bh = tf.Variable(tf.zeros([hidden_layer]), dtype=tf.float32)\n",
    "by = tf.Variable(tf.zeros([output_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    h = tf.tanh((tf.matmul(p, Whh))  + tf.matmul(i, Wxh) + bh)\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tf.identity(cnn_outputs, name='lstm_in')\n",
    "# b = tf.identity(cnn_outputs, name='lstm_out')\n",
    "b = tf.placeholder(shape=[batch_size, vocab_size], dtype=tf.float32, name='targets')\n",
    "\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b * tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "# for g,v in grad_var_pairs:\n",
    "#     print(g, v.name)\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 15, 87, 1)\n",
      "(5, 1500)\n",
      "36.5687\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "ix = 0\n",
    "feed = {input_: sequence_char_matrix(ix, batch_size), initial: np.zeros(hidden_layer), b: word_generator(ix, batch_size)}\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print(sess.run(outputs, feed_dict=feed).shape)\n",
    "print(sess.run(loss, feed_dict=feed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
