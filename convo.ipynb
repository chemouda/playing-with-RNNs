{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1909\n",
      "12063\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1500\n",
    "d, id_to_token = data.get_data(vocab_size)\n",
    "for k in id_to_token:\n",
    "    if id_to_token[k] == 'eos':\n",
    "        id_to_token[k] = '\\n'\n",
    "print(len(d))\n",
    "x, y = d[10:20], d[11:21]\n",
    "max_word_length = max([len(id_to_token[np.argmax(j)]) for j in d])\n",
    "print(max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 7, 71]\n",
      "[8, 10, 44, 7, 50, 47, 23]\n",
      "(1, 15, 87, 1) (1, 15, 87, 1)\n"
     ]
    }
   ],
   "source": [
    "c_to_i, embedding = data.get_char_embedding()\n",
    "second_word = id_to_token[np.argmax(x[1])]\n",
    "third_word = id_to_token[np.argmax(x[2])]\n",
    "chars2 = [c_to_i[c] for c in second_word]\n",
    "chars3 = [c_to_i[c] for c in third_word]\n",
    "print(chars2)\n",
    "print(chars3)\n",
    "embed2 = embedding[chars2]\n",
    "embed3 = embedding[chars3]\n",
    "to_pad2 = max_word_length - len(chars2)\n",
    "to_pad3 = max_word_length - len(chars3)\n",
    "pad2 = np.zeros([to_pad2, embedding.shape[0]])\n",
    "pad3 = np.zeros([to_pad3, embedding.shape[0]])\n",
    "reshape = [1, max_word_length, embedding.shape[0], 1]\n",
    "padded2 = np.reshape(np.r_[embed2, pad2], reshape)\n",
    "padded3 = np.reshape(np.r_[embed3, pad3], reshape)\n",
    "print(padded2.shape, padded3.shape)\n",
    "words = [padded2, padded2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 87\n"
     ]
    }
   ],
   "source": [
    "input_height = max_word_length\n",
    "input_width = embedding.shape[0]\n",
    "batch_size = 2\n",
    "print(input_height, input_width)\n",
    "filter_heights = [2, 3, 4]\n",
    "feature_maps = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(inp):\n",
    "    with tf.variable_scope('conv1') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[0], input_width, 1, feature_maps[0]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv1 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool1 = tf.squeeze(tf.nn.max_pool(conv1, ksize=[1, conv1.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[1], input_width, 1, feature_maps[1]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv2 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool2 = tf.squeeze(tf.nn.max_pool(conv2, ksize=[1, conv2.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    with tf.variable_scope('conv3') as scope:        \n",
    "        weight = tf.get_variable('weights', [filter_heights[2], input_width, 1, feature_maps[2]], initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05))\n",
    "        conv3 = tf.nn.tanh(tf.nn.conv2d(inp, weight, strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    pool3 = tf.squeeze(tf.nn.max_pool(conv3, ksize=[1, conv3.get_shape()[1], 1, 1], strides=[1, 1, 1, 1], padding='VALID'))\n",
    "    \n",
    "    pool_total = tf.concat(0, [pool1, pool2, pool3], name='total_pool')\n",
    "    return pool_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "input_ = tf.placeholder(shape=[batch_size, max_word_length, embedding.shape[0], 1], dtype=tf.float32, name = 'cnn_in')\n",
    "words = tf.split(0, batch_size, input_)\n",
    "cnn_outputs = []\n",
    "with tf.variable_scope(\"CNN\") as scope:\n",
    "    for idx, word in enumerate(words):\n",
    "#         inp = tf.Variable(word, trainable=False, dtype=tf.float32)\n",
    "        if idx != 0:\n",
    "            scope.reuse_variables()\n",
    "        p = inference(word)\n",
    "        cnn_outputs.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "epochs = 5000\n",
    "hidden_layer = 128\n",
    "inp_out_size = sum(feature_maps)\n",
    "learning_rate = 0.1\n",
    "num_steps = 15\n",
    "inp_out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Wxh = tf.Variable(tf.random_uniform([inp_out_size, hidden_layer], minval=-0.05, maxval=0.05))\n",
    "Whh = tf.Variable(tf.random_uniform([hidden_layer, hidden_layer], minval=-0.05, maxval=0.05))\n",
    "Why = tf.Variable(tf.random_uniform([hidden_layer, inp_out_size], minval=-0.05, maxval=0.05))\n",
    "bh = tf.Variable(tf.zeros([hidden_layer]), dtype=tf.float32)\n",
    "by = tf.Variable(tf.zeros([inp_out_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recurrence(prev, inp):\n",
    "    i = tf.reshape(inp, shape=[1, -1])\n",
    "    p = tf.reshape(prev, shape=[1, -1])\n",
    "    h = tf.tanh((tf.matmul(p, Whh))  + tf.matmul(i, Wxh) + bh)\n",
    "    h = tf.reshape(h, [hidden_layer])\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "# b = tf.placeholder(shape=[None, inp_out_size], dtype=tf.float32)\n",
    "a = tf.identity(cnn_outputs, name='lstm_in')\n",
    "b = tf.identity(cnn_outputs, name='lstm_out')\n",
    "\n",
    "initial = tf.placeholder(shape=[hidden_layer], dtype=tf.float32)\n",
    "states = tf.scan(recurrence, a, initializer=initial)\n",
    "outputs = tf.nn.softmax(tf.matmul(states, Why) + by)\n",
    "loss = -tf.reduce_sum(b * tf.log(outputs))\n",
    "# loss = tf.sqrt(tf.reduce_sum(tf.square(tf.sub(outputs, b))))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "\n",
    "# clipping gradients between -1 and 1.\n",
    "grad_var_pairs = optimizer.compute_gradients(loss, tf.trainable_variables())\n",
    "# for g,v in grad_var_pairs:\n",
    "#     print(g, v.name)\n",
    "clipped_grad_var_pairs = [(tf.clip_by_value(gv[0], -1, 1), gv[1]) for gv in grad_var_pairs]\n",
    "optimize_op = optimizer.apply_gradients(clipped_grad_var_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 15, 87, 1)\n",
      "[[ 0.08330277  0.08346034  0.08336122  0.08314192  0.08326701  0.08319741\n",
      "   0.08330587  0.08340639  0.08341812  0.08336986  0.08331613  0.08345299]\n",
      " [ 0.08335734  0.08348259  0.083432    0.08303896  0.08325277  0.08302888\n",
      "   0.08319126  0.08354958  0.08350077  0.08339152  0.08336149  0.08341287]]\n",
      "2.82418\n"
     ]
    }
   ],
   "source": [
    "padded = np.concatenate((padded2, padded3), axis = 0)\n",
    "print(padded.shape)\n",
    "sess = tf.Session()\n",
    "feed = {input_: padded, initial: np.zeros(hidden_layer)}\n",
    "sess.run(tf.initialize_all_variables())\n",
    "print(sess.run(outputs, feed_dict=feed))\n",
    "print(sess.run(loss, feed_dict=feed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
